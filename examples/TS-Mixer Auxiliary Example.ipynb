{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Description\n",
    "\n",
    "This notebook only illustrates the workings of the auxiliary TS_Mixer model including static and dynamic features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from TSMixer.model_auxiliary import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "fcst_h = 20\n",
    "x = torch.randn(100,3)[None,:]\n",
    "z = torch.randn(fcst_h,2)[None,:]\n",
    "s = torch.ones_like(z)\n",
    "s[:,:, 1] = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "n_ts = x.shape[2]\n",
    "n_static_feat = s.shape[2]\n",
    "n_dynamic_feat = z.shape[2]\n",
    "ts_length = x.shape[1]\n",
    "embed_dim = 64\n",
    "\n",
    "n_feat_sx = embed_dim + n_ts\n",
    "n_feat_sz = embed_dim + n_dynamic_feat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 20, 3]),\n torch.Size([1, 20, 64]),\n torch.Size([1, 20, 64]),\n torch.Size([1, 20, 64]),\n torch.Size([1, 20, 64]),\n torch.Size([1, 20, 128]),\n torch.Size([1, 20, 64]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_x_map = nn.Linear(ts_length, fcst_h)\n",
    "x_map = mlp_x_map(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "mlp_sx = MLP_Feat(n_static_feat, embed_dim, dropout=0.1)\n",
    "sx_out = mlp_sx(s)\n",
    "\n",
    "mlp_sz = MLP_Feat(n_dynamic_feat, embed_dim, dropout=0.1)\n",
    "sz_out = mlp_sz(s)\n",
    "\n",
    "mlp_x = MLP_Feat(n_feat_sx, embed_dim, dropout=0.1)\n",
    "x_prime = mlp_x(torch.cat([x_map, sx_out], dim=2))\n",
    "\n",
    "mlp_z = MLP_Feat(n_feat_sz, embed_dim, dropout=0.1)\n",
    "z_prime = mlp_z(torch.cat([z, sz_out], dim=2))\n",
    "\n",
    "y_prime = torch.cat([x_prime, z_prime], dim=2)\n",
    "\n",
    "n_feat = y_prime.shape[2]\n",
    "\n",
    "mixer_block = Mixer_Block(n_feat, n_static_feat, fcst_h, embed_dim, dropout=0.1)\n",
    "y_prime_mixer = mixer_block(y_prime, s)\n",
    "\n",
    "x_map.shape, sx_out.shape, sz_out.shape, x_prime.shape, z_prime.shape, y_prime.shape, y_prime_mixer.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TS-Mixer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 20, 3])\n",
      "x_prime: torch.Size([1, 20, 64])\n",
      "z_prime: torch.Size([1, 20, 64])\n",
      "y_prime: torch.Size([1, 20, 128])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m ts_mixer_aux \u001B[38;5;241m=\u001B[39m TS_Mixer_auxiliary(n_ts, n_static_feat, n_dynamic_feat, ts_length, embed_dim, num_blocks, fcst_h, out_dim, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# ts_mixer_aux\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mts_mixer_aux\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\ConvTS_Mixer-3Rl3B8jo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive - adidas\\ConvTS Mixer\\TSMixer\\model_auxiliary.py:173\u001B[0m, in \u001B[0;36mTS_Mixer_auxiliary.forward\u001B[1;34m(self, x, z, s)\u001B[0m\n\u001B[0;32m    170\u001B[0m y_prime \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([x_prime, z_prime], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_prime: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my_prime\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 173\u001B[0m y_prime \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmixer_blocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_prime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmixer_block: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my_prime\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    176\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm(\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_out(y_prime)\n\u001B[0;32m    178\u001B[0m )\n",
      "File \u001B[1;32m~\\.virtualenvs\\ConvTS_Mixer-3Rl3B8jo\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[1;31mTypeError\u001B[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "num_blocks=1\n",
    "out_dim=2\n",
    "ts_mixer_aux = TS_Mixer_auxiliary(n_ts, n_static_feat, n_dynamic_feat, ts_length, embed_dim, num_blocks, fcst_h, out_dim, dropout=0.1)\n",
    "# ts_mixer_aux\n",
    "out = ts_mixer_aux(x, z, s)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
